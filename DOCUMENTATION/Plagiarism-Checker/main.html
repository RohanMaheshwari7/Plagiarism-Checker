<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>Plagiarism-Checker.main API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Plagiarism-Checker.main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import os
import sys
import math
import numpy as np
#ensure you have Python 3.6+

def extractParas(text):
        &#34;&#34;&#34;
        Extract paras from the raw text provided

        Parameters:  
        str: raw text data from the input file  

        Returns:  
        list: List of paragraphs from the input document  
        &#34;&#34;&#34;

        paragraphs = []
        para = &#34;&#34;
        i = 0
        while i &lt; len(text):
                if i &lt; len(text) and text[i] == &#39;\n&#39;:
                        if para != &#34;&#34;:
                                paragraphs.append(para)
                        para = &#34;&#34;
                else:
                        para += text[i]
                i += 1
        if para != &#34;\n&#34; and para != &#34;&#34;:
                paragraphs.append(para)

        return paragraphs

def tokenize(text):
        &#34;&#34;&#34;
        Tokenize the given paragraph

        Parameters:  
        list: List of paragraphs from the input document  

        Returns:  
        list: List of tokens from the input paragraph  
        &#34;&#34;&#34;

        tokenizer = RegexpTokenizer(r&#39;\w+&#39;)
        tokens = tokenizer.tokenize(text)

        #Convert to lower case
        for i, token in enumerate(tokens):
                tokens[i] = token.lower()
        return tokens

def stopwordRemoval(tokens):
        &#34;&#34;&#34;
        Remove common English stopwords from the tokens
        
        Parameters:  
        list: List of tokens  

        Returns:  
        list: List of tokens without the stop words  
        &#34;&#34;&#34;

        stop_words = set(stopwords.words(&#39;english&#39;))
        tokens = [w for w in tokens if not w in stop_words]
        return tokens

def stemmer(tokens):
        &#34;&#34;&#34;
        Stemming function

        Parameters:  
        list: List of tokens without stopwords  

        Returns:  
        list: List of stemmed tokens  
        &#34;&#34;&#34;
        stemmer = PorterStemmer()
        for i, token in enumerate(tokens):
                tokens[i] = stemmer.stem(token)
        return tokens

def createInvertedIndex(documents):
        &#34;&#34;&#34;
        Creating the inverted index

        Parameters:  
        documents(list): Preprocessed list containing content of the documents  

        Returns:  
        Dictionary: The inverted index  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: posting list  
        &#34;&#34;&#34;

        inverted_index = {}
        for i,doc in enumerate(documents):
                for j, para in enumerate(doc):
                        for word in para:
                                # print(word)
                                if inverted_index.get(word,False):
                                        if i not in inverted_index[word]:
                                                inverted_index[word].append(i)
                                else:
                                        inverted_index[word] = [i]
        return inverted_index


def calculate_TfIdf_Weights(vocab, inverted_index, documents):
        &#34;&#34;&#34;
        Calculate the tfidf scores 

        Parameters:  
        (arg1)vocab(set): Vocabulary of training data  
        (arg2)inverted_index(dict): Inverted Index  
        (arg3)documents(list): Preprocessed list containing content of the documents  


        Returns:  
        Dictionary: TfIdf scores  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: list of tf * idf scores for all the documents  
        &#34;&#34;&#34;
        
        tf_idf = {}
        count = 0
        for word in vocab:
                for i,doc in enumerate(documents):
                        if i in inverted_index[word]:
                                for para in doc:
                                        for w in para:
                                                if(w == word):
                                                        count+=1
                        else:
                                count = 0

                        if (i != 0):
                                tf_idf[word].append(count)
                        else:
                                tf_idf[word] = [count]
                        count = 0

        #convert to 1 + log(tf)
        for word in vocab:
                tf_idf[word] = [(1 + math.log(x+1)) for x in tf_idf[word]]

        #add idf weighting
        totaldocs = len(documents)
        for word in vocab:
                idfval = math.log(totaldocs/len(inverted_index[word]))
                tf_idf[word] = [round(x * idfval, 3) for x in tf_idf[word]]

        return tf_idf


def rankDocsByMatchingSimilarity(documents, testparagraphs, tf_idf):
        &#34;&#34;&#34;
        Calculate the ranking of the docs wrt matching similarity to testdoc 

        Parameters:  
        (arg1)documents(list): Preprocessed list containing content of the documents  
        (arg2)testparagraphs(list): paragraphs extracted from test document  
        (arg3)tf_idf(dict): TfIdf scores  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: list of tf * idf scores for all the documents  

        Returns:  
        Dictionary: Ranking  
        &amp;nbsp;&amp;nbsp; Key: doc number  
        &amp;nbsp;&amp;nbsp; Value: Similarity score  
        &#34;&#34;&#34;

        ranking = {} 

        for i,doc in enumerate(documents):
                ranking[i] = 0
                for j,para in enumerate(testparagraphs):
                        for word in para:
                                ranking[i] += tf_idf[word][i]
                ranking[i] = round(ranking[i],3)
        
        #sort ranking dict in reverse order by keys
        ranking = {k: v for k, v in sorted(ranking.items(),reverse=True, key=lambda item: item[1])}
        return ranking


def calculateQueryWeights(testvocab, inverted_index, testparagraphs, totaldocs):
        &#34;&#34;&#34;
        Calculate weight of the query passed - here query is a document 

        Parameters:  
        arg1: testvocab(set): Preprocessed list containing content of the documents  
        arg2: inverted_index(dict): Inverted Index  
        arg3: testparagraphs(list): paragraphs extracted from test document  
        arg4: totaldocs(int): Number of documents in Training corpus  

        Returns:  
        weights(dictionary)  
        &amp;nbsp;&amp;nbsp; Key: word in test document vocabulary  
        &amp;nbsp;&amp;nbsp; Value: TfIdf score  
        &#34;&#34;&#34;
        weights = {}
        
        for word in testvocab:
                count = 0
                for para in testparagraphs:
                        for w in para:
                                if(w == word):
                                        count += 1
                weights[word] = count

        for word in testvocab:
                weights[word] = 1 + math.log(weights[word]+1)

        
        for word in testvocab:
                if word not in inverted_index:
                        inverted_index[word] = []
                        tf_idf[word] = [0]*totaldocs
                idfval = math.log((totaldocs+1)/(len(inverted_index[word])+1))
                
                weights[word] = round(weights[word] * idfval, 3)

        return weights

def calculateParaWeights(para, vocab, inverted_index, totaldocs):
        &#34;&#34;&#34;
        Calculate weight of the query passed - here query is a paragraph 

        Parameters:  
        arg1: para(list): List of tokens in the query paragraph  
        arg2: testvocab(set): Preprocessed list containing content of the documents  
        arg3: inverted_index(dict): Inverted Index  
        arg4: totaldocs(int): Number of documents in Training corpus  

        Returns:  
        weights(numpy array) - contains weights of the words in the test para vocabulary for the passed para    
        &#34;&#34;&#34;

        weights = np.zeros(len(vocab))
        for i,word in enumerate(vocab):
                count = 0
                for w in para:
                        if(w == word):
                                count += 1
                weights[i] = count

                for i in range(len(vocab)):
                        if(weights[i] != 0):
                                weights[i] = 1 + math.log(weights[i]+1)

                for i,word in enumerate(vocab):
                        if word not in inverted_index:
                                inverted_index[word] = []
                                tf_idf[word] = [0]*totaldocs
                        idfval = math.log((totaldocs+1)/(len(inverted_index[word])+1))
                        weights[i] = round(weights[i] * idfval, 3)
        return weights


def rankDocsByCosineSimilarity(documents, testparagraphs, tf_idf, inverted_index):
        ranking = {}
        testvocab = set()
        for para in testparagraphs:
                for word in para:
                        testvocab.add(word)
        testdoc = []
        testdoc.append(testparagraphs)
        query_tfidf = calculateQueryWeights(testvocab, inverted_index, testparagraphs, len(documents))
        a = np.zeros(len(testvocab))
        b = np.zeros(len(testvocab))

        # print(query_tfidf)
        
        for j,doc in enumerate(documents):
                i = 0
                for word in testvocab:
                        if word in inverted_index and j in inverted_index[word]:
                                a[i] = tf_idf[word][j]
                        else:
                                a[i] = 0
                        b[i] = query_tfidf[word]
                        i += 1
                ranking[j] = cosine_sim(a,b)

        #sort ranking dict in reverse order by keys
        ranking = {k: v for k, v in sorted(ranking.items(),reverse = True, key=lambda item: item[1])}
        # print(ranking)
        return ranking


def cosine_sim(a,b):
        &#34;&#34;&#34;
        Calculate cosine of two vectors
        
        Parameters:  
        arg1: a(list) - 1st vector
        arg2: b(list) - 2nd vector

        Return:
        cos_sim(float) - Computed cosine
        &#34;&#34;&#34;

        cos_sim = 0
        if np.linalg.norm(a)!=0 and np.linalg.norm(b)!=0:
                cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
        return cos_sim




if __name__ == &#39;__main__&#39;:
        documents=[]
        filename = {}
        vocab = set()
        files = os.listdir(&#39;TRAIN&#39;)
        for i,file in enumerate(files):
                filename[i] = file
                with open(&#39;TRAIN/&#39; + file, encoding=&#34;utf8&#34;, errors=&#39;ignore&#39;) as f:
                        data = f.read()
                        paras = extractParas(data)
                        paragraphs = []
                        for j,para in enumerate(paras):
                                ###preprocessing###
                                tokens = tokenize(para)
                                stoplesstokens = stopwordRemoval(tokens)
                                finaltokens = stemmer(stoplesstokens)
                                ###
                                paragraphs.append(finaltokens)
                                for term in finaltokens:
                                        vocab.add(term)
                        documents.append(paragraphs)
        
        vocablen = len(vocab)

        doclen = [] #length of the docs
        totcount = 0
        for doc in documents:
                totcount = 0
                for para in doc:
                        totcount += len(para)
                doclen.append(totcount)



        inverted_index = createInvertedIndex(documents)
        

        tf_idf = calculate_TfIdf_Weights(vocab, inverted_index, documents)



        #take testfile name from cl arguments
        testDocument = str(sys.argv[1])

        with open(testDocument, encoding=&#34;utf8&#34;, errors=&#34;ignore&#34;) as input_file:
                testdata = input_file.read()

        #process test document
        paras = extractParas(testdata)
        testparagraphs = []
        for j, para in enumerate(paras):
                ###preprocessing###
                tokens = tokenize(para)
                stoplesstokens = stopwordRemoval(tokens)
                finaltokens = stemmer(stoplesstokens)
                ###
                testparagraphs.append(finaltokens)
        
        # print(len(testparagraphs), testparagraphs)
        
        

        # ranking = rankDocsByMatchingSimilarity(documents, testparagraphs, tf_idf)
        # Use when Matching Similarity is to be used to rank docs

        ranking = rankDocsByCosineSimilarity(documents, testparagraphs, tf_idf, inverted_index)

        #print top 10 matching documents
        print(&#34;\nTop 10 documents matching the given test document in ranked order are: &#34;)
        print(&#34;Rank&#34;,&#34;  -       &#34;,&#34;Doc No&#34;, &#34;   -       &#34;,&#34;Doc Name&#34;,&#34;          -       &#34;,&#34;Cosine Score&#34;)
        cnt = 0
        for i in ranking:
                cnt += 1
                print(cnt,&#34;     -       &#34;,i, &#34;          -       &#34;,filename[i],&#34; -       &#34;,ranking[i])
                if(cnt&gt;10):
                        break

        #para as a query part
        print(&#34;\n&#34;)
        
        totaldocs = len(documents)
        totalmatches = 0
        for k, tpara in enumerate(testparagraphs):
                testparavocab = set()
                for term in tpara:
                        testparavocab.add(term)
                testparaweights = calculateParaWeights(tpara, testparavocab, inverted_index, totaldocs)
                matchfound = 0
                for i,doc in enumerate(documents):
                        for j,para in enumerate(doc):
                                paraweights = calculateParaWeights(para, testparavocab, inverted_index, totaldocs)
                                cos_sim = cosine_sim(testparaweights, paraweights)
                                if(cos_sim &gt; 0.95):
                                        matchfound = 1
                                        totalmatches += 1
                                        print(&#34;Paragraph &#34;,k,&#34; from test document matches with paragraph &#34;,j,&#34; from document &#34;,i,&#34; - &#34;,filename[i])
                                        break
                        if(matchfound == 1):
                                break
        print(&#34;Document uniqueness = &#34;, round(100 - (totalmatches*100/len(testparagraphs)), 2),&#34;%&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Plagiarism-Checker.main.calculateParaWeights"><code class="name flex">
<span>def <span class="ident">calculateParaWeights</span></span>(<span>para, vocab, inverted_index, totaldocs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate weight of the query passed - here query is a paragraph </p>
<p>Parameters:<br>
arg1: para(list): List of tokens in the query paragraph<br>
arg2: testvocab(set): Preprocessed list containing content of the documents<br>
arg3: inverted_index(dict): Inverted Index<br>
arg4: totaldocs(int): Number of documents in Training corpus
</p>
<p>Returns:<br>
weights(numpy array) - contains weights of the words in the test para vocabulary for the passed para</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculateParaWeights(para, vocab, inverted_index, totaldocs):
        &#34;&#34;&#34;
        Calculate weight of the query passed - here query is a paragraph 

        Parameters:  
        arg1: para(list): List of tokens in the query paragraph  
        arg2: testvocab(set): Preprocessed list containing content of the documents  
        arg3: inverted_index(dict): Inverted Index  
        arg4: totaldocs(int): Number of documents in Training corpus  

        Returns:  
        weights(numpy array) - contains weights of the words in the test para vocabulary for the passed para    
        &#34;&#34;&#34;

        weights = np.zeros(len(vocab))
        for i,word in enumerate(vocab):
                count = 0
                for w in para:
                        if(w == word):
                                count += 1
                weights[i] = count

                for i in range(len(vocab)):
                        if(weights[i] != 0):
                                weights[i] = 1 + math.log(weights[i]+1)

                for i,word in enumerate(vocab):
                        if word not in inverted_index:
                                inverted_index[word] = []
                                tf_idf[word] = [0]*totaldocs
                        idfval = math.log((totaldocs+1)/(len(inverted_index[word])+1))
                        weights[i] = round(weights[i] * idfval, 3)
        return weights</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.calculateQueryWeights"><code class="name flex">
<span>def <span class="ident">calculateQueryWeights</span></span>(<span>testvocab, inverted_index, testparagraphs, totaldocs)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate weight of the query passed - here query is a document </p>
<p>Parameters:<br>
arg1: testvocab(set): Preprocessed list containing content of the documents<br>
arg2: inverted_index(dict): Inverted Index<br>
arg3: testparagraphs(list): paragraphs extracted from test document<br>
arg4: totaldocs(int): Number of documents in Training corpus
</p>
<p>Returns:<br>
weights(dictionary)<br>
&nbsp;&nbsp; Key: word in test document vocabulary<br>
&nbsp;&nbsp; Value: TfIdf score</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculateQueryWeights(testvocab, inverted_index, testparagraphs, totaldocs):
        &#34;&#34;&#34;
        Calculate weight of the query passed - here query is a document 

        Parameters:  
        arg1: testvocab(set): Preprocessed list containing content of the documents  
        arg2: inverted_index(dict): Inverted Index  
        arg3: testparagraphs(list): paragraphs extracted from test document  
        arg4: totaldocs(int): Number of documents in Training corpus  

        Returns:  
        weights(dictionary)  
        &amp;nbsp;&amp;nbsp; Key: word in test document vocabulary  
        &amp;nbsp;&amp;nbsp; Value: TfIdf score  
        &#34;&#34;&#34;
        weights = {}
        
        for word in testvocab:
                count = 0
                for para in testparagraphs:
                        for w in para:
                                if(w == word):
                                        count += 1
                weights[word] = count

        for word in testvocab:
                weights[word] = 1 + math.log(weights[word]+1)

        
        for word in testvocab:
                if word not in inverted_index:
                        inverted_index[word] = []
                        tf_idf[word] = [0]*totaldocs
                idfval = math.log((totaldocs+1)/(len(inverted_index[word])+1))
                
                weights[word] = round(weights[word] * idfval, 3)

        return weights</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.calculate_TfIdf_Weights"><code class="name flex">
<span>def <span class="ident">calculate_TfIdf_Weights</span></span>(<span>vocab, inverted_index, documents)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the tfidf scores </p>
<p>Parameters:<br>
(arg1)vocab(set): Vocabulary of training data<br>
(arg2)inverted_index(dict): Inverted Index<br>
(arg3)documents(list): Preprocessed list containing content of the documents
</p>
<p>Returns:<br>
Dictionary: TfIdf scores<br>
&nbsp;&nbsp; Key: word<br>
&nbsp;&nbsp; Value: list of tf * idf scores for all the documents</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_TfIdf_Weights(vocab, inverted_index, documents):
        &#34;&#34;&#34;
        Calculate the tfidf scores 

        Parameters:  
        (arg1)vocab(set): Vocabulary of training data  
        (arg2)inverted_index(dict): Inverted Index  
        (arg3)documents(list): Preprocessed list containing content of the documents  


        Returns:  
        Dictionary: TfIdf scores  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: list of tf * idf scores for all the documents  
        &#34;&#34;&#34;
        
        tf_idf = {}
        count = 0
        for word in vocab:
                for i,doc in enumerate(documents):
                        if i in inverted_index[word]:
                                for para in doc:
                                        for w in para:
                                                if(w == word):
                                                        count+=1
                        else:
                                count = 0

                        if (i != 0):
                                tf_idf[word].append(count)
                        else:
                                tf_idf[word] = [count]
                        count = 0

        #convert to 1 + log(tf)
        for word in vocab:
                tf_idf[word] = [(1 + math.log(x+1)) for x in tf_idf[word]]

        #add idf weighting
        totaldocs = len(documents)
        for word in vocab:
                idfval = math.log(totaldocs/len(inverted_index[word]))
                tf_idf[word] = [round(x * idfval, 3) for x in tf_idf[word]]

        return tf_idf</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.cosine_sim"><code class="name flex">
<span>def <span class="ident">cosine_sim</span></span>(<span>a, b)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate cosine of two vectors</p>
<p>Parameters:<br>
arg1: a(list) - 1st vector
arg2: b(list) - 2nd vector</p>
<p>Return:
cos_sim(float) - Computed cosine</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_sim(a,b):
        &#34;&#34;&#34;
        Calculate cosine of two vectors
        
        Parameters:  
        arg1: a(list) - 1st vector
        arg2: b(list) - 2nd vector

        Return:
        cos_sim(float) - Computed cosine
        &#34;&#34;&#34;

        cos_sim = 0
        if np.linalg.norm(a)!=0 and np.linalg.norm(b)!=0:
                cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
        return cos_sim</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.createInvertedIndex"><code class="name flex">
<span>def <span class="ident">createInvertedIndex</span></span>(<span>documents)</span>
</code></dt>
<dd>
<div class="desc"><p>Creating the inverted index</p>
<p>Parameters:<br>
documents(list): Preprocessed list containing content of the documents
</p>
<p>Returns:<br>
Dictionary: The inverted index<br>
&nbsp;&nbsp; Key: word<br>
&nbsp;&nbsp; Value: posting list</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createInvertedIndex(documents):
        &#34;&#34;&#34;
        Creating the inverted index

        Parameters:  
        documents(list): Preprocessed list containing content of the documents  

        Returns:  
        Dictionary: The inverted index  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: posting list  
        &#34;&#34;&#34;

        inverted_index = {}
        for i,doc in enumerate(documents):
                for j, para in enumerate(doc):
                        for word in para:
                                # print(word)
                                if inverted_index.get(word,False):
                                        if i not in inverted_index[word]:
                                                inverted_index[word].append(i)
                                else:
                                        inverted_index[word] = [i]
        return inverted_index</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.extractParas"><code class="name flex">
<span>def <span class="ident">extractParas</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract paras from the raw text provided</p>
<p>Parameters:<br>
str: raw text data from the input file
</p>
<p>Returns:<br>
list: List of paragraphs from the input document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extractParas(text):
        &#34;&#34;&#34;
        Extract paras from the raw text provided

        Parameters:  
        str: raw text data from the input file  

        Returns:  
        list: List of paragraphs from the input document  
        &#34;&#34;&#34;

        paragraphs = []
        para = &#34;&#34;
        i = 0
        while i &lt; len(text):
                if i &lt; len(text) and text[i] == &#39;\n&#39;:
                        if para != &#34;&#34;:
                                paragraphs.append(para)
                        para = &#34;&#34;
                else:
                        para += text[i]
                i += 1
        if para != &#34;\n&#34; and para != &#34;&#34;:
                paragraphs.append(para)

        return paragraphs</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.rankDocsByCosineSimilarity"><code class="name flex">
<span>def <span class="ident">rankDocsByCosineSimilarity</span></span>(<span>documents, testparagraphs, tf_idf, inverted_index)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rankDocsByCosineSimilarity(documents, testparagraphs, tf_idf, inverted_index):
        ranking = {}
        testvocab = set()
        for para in testparagraphs:
                for word in para:
                        testvocab.add(word)
        testdoc = []
        testdoc.append(testparagraphs)
        query_tfidf = calculateQueryWeights(testvocab, inverted_index, testparagraphs, len(documents))
        a = np.zeros(len(testvocab))
        b = np.zeros(len(testvocab))

        # print(query_tfidf)
        
        for j,doc in enumerate(documents):
                i = 0
                for word in testvocab:
                        if word in inverted_index and j in inverted_index[word]:
                                a[i] = tf_idf[word][j]
                        else:
                                a[i] = 0
                        b[i] = query_tfidf[word]
                        i += 1
                ranking[j] = cosine_sim(a,b)

        #sort ranking dict in reverse order by keys
        ranking = {k: v for k, v in sorted(ranking.items(),reverse = True, key=lambda item: item[1])}
        # print(ranking)
        return ranking</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.rankDocsByMatchingSimilarity"><code class="name flex">
<span>def <span class="ident">rankDocsByMatchingSimilarity</span></span>(<span>documents, testparagraphs, tf_idf)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate the ranking of the docs wrt matching similarity to testdoc </p>
<p>Parameters:<br>
(arg1)documents(list): Preprocessed list containing content of the documents<br>
(arg2)testparagraphs(list): paragraphs extracted from test document<br>
(arg3)tf_idf(dict): TfIdf scores<br>
&nbsp;&nbsp; Key: word<br>
&nbsp;&nbsp; Value: list of tf * idf scores for all the documents
</p>
<p>Returns:<br>
Dictionary: Ranking<br>
&nbsp;&nbsp; Key: doc number<br>
&nbsp;&nbsp; Value: Similarity score</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rankDocsByMatchingSimilarity(documents, testparagraphs, tf_idf):
        &#34;&#34;&#34;
        Calculate the ranking of the docs wrt matching similarity to testdoc 

        Parameters:  
        (arg1)documents(list): Preprocessed list containing content of the documents  
        (arg2)testparagraphs(list): paragraphs extracted from test document  
        (arg3)tf_idf(dict): TfIdf scores  
        &amp;nbsp;&amp;nbsp; Key: word  
        &amp;nbsp;&amp;nbsp; Value: list of tf * idf scores for all the documents  

        Returns:  
        Dictionary: Ranking  
        &amp;nbsp;&amp;nbsp; Key: doc number  
        &amp;nbsp;&amp;nbsp; Value: Similarity score  
        &#34;&#34;&#34;

        ranking = {} 

        for i,doc in enumerate(documents):
                ranking[i] = 0
                for j,para in enumerate(testparagraphs):
                        for word in para:
                                ranking[i] += tf_idf[word][i]
                ranking[i] = round(ranking[i],3)
        
        #sort ranking dict in reverse order by keys
        ranking = {k: v for k, v in sorted(ranking.items(),reverse=True, key=lambda item: item[1])}
        return ranking</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.stemmer"><code class="name flex">
<span>def <span class="ident">stemmer</span></span>(<span>tokens)</span>
</code></dt>
<dd>
<div class="desc"><p>Stemming function</p>
<p>Parameters:<br>
list: List of tokens without stopwords
</p>
<p>Returns:<br>
list: List of stemmed tokens</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stemmer(tokens):
        &#34;&#34;&#34;
        Stemming function

        Parameters:  
        list: List of tokens without stopwords  

        Returns:  
        list: List of stemmed tokens  
        &#34;&#34;&#34;
        stemmer = PorterStemmer()
        for i, token in enumerate(tokens):
                tokens[i] = stemmer.stem(token)
        return tokens</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.stopwordRemoval"><code class="name flex">
<span>def <span class="ident">stopwordRemoval</span></span>(<span>tokens)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove common English stopwords from the tokens</p>
<p>Parameters:<br>
list: List of tokens
</p>
<p>Returns:<br>
list: List of tokens without the stop words</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stopwordRemoval(tokens):
        &#34;&#34;&#34;
        Remove common English stopwords from the tokens
        
        Parameters:  
        list: List of tokens  

        Returns:  
        list: List of tokens without the stop words  
        &#34;&#34;&#34;

        stop_words = set(stopwords.words(&#39;english&#39;))
        tokens = [w for w in tokens if not w in stop_words]
        return tokens</code></pre>
</details>
</dd>
<dt id="Plagiarism-Checker.main.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>Tokenize the given paragraph</p>
<p>Parameters:<br>
list: List of paragraphs from the input document
</p>
<p>Returns:<br>
list: List of tokens from the input paragraph</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize(text):
        &#34;&#34;&#34;
        Tokenize the given paragraph

        Parameters:  
        list: List of paragraphs from the input document  

        Returns:  
        list: List of tokens from the input paragraph  
        &#34;&#34;&#34;

        tokenizer = RegexpTokenizer(r&#39;\w+&#39;)
        tokens = tokenizer.tokenize(text)

        #Convert to lower case
        for i, token in enumerate(tokens):
                tokens[i] = token.lower()
        return tokens</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Plagiarism-Checker" href="index.html">Plagiarism-Checker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="Plagiarism-Checker.main.calculateParaWeights" href="#Plagiarism-Checker.main.calculateParaWeights">calculateParaWeights</a></code></li>
<li><code><a title="Plagiarism-Checker.main.calculateQueryWeights" href="#Plagiarism-Checker.main.calculateQueryWeights">calculateQueryWeights</a></code></li>
<li><code><a title="Plagiarism-Checker.main.calculate_TfIdf_Weights" href="#Plagiarism-Checker.main.calculate_TfIdf_Weights">calculate_TfIdf_Weights</a></code></li>
<li><code><a title="Plagiarism-Checker.main.cosine_sim" href="#Plagiarism-Checker.main.cosine_sim">cosine_sim</a></code></li>
<li><code><a title="Plagiarism-Checker.main.createInvertedIndex" href="#Plagiarism-Checker.main.createInvertedIndex">createInvertedIndex</a></code></li>
<li><code><a title="Plagiarism-Checker.main.extractParas" href="#Plagiarism-Checker.main.extractParas">extractParas</a></code></li>
<li><code><a title="Plagiarism-Checker.main.rankDocsByCosineSimilarity" href="#Plagiarism-Checker.main.rankDocsByCosineSimilarity">rankDocsByCosineSimilarity</a></code></li>
<li><code><a title="Plagiarism-Checker.main.rankDocsByMatchingSimilarity" href="#Plagiarism-Checker.main.rankDocsByMatchingSimilarity">rankDocsByMatchingSimilarity</a></code></li>
<li><code><a title="Plagiarism-Checker.main.stemmer" href="#Plagiarism-Checker.main.stemmer">stemmer</a></code></li>
<li><code><a title="Plagiarism-Checker.main.stopwordRemoval" href="#Plagiarism-Checker.main.stopwordRemoval">stopwordRemoval</a></code></li>
<li><code><a title="Plagiarism-Checker.main.tokenize" href="#Plagiarism-Checker.main.tokenize">tokenize</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>